services:
  # Frontend - React application
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:80"
    environment:
      - NODE_ENV=production
    depends_on:
      - api-gateway
    networks:
      - youtube-transcriber
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # API Gateway - Express Gateway with gRPC client for workflow service
  api-gateway:
    build:
      context: .
      dockerfile: ./gateway/Dockerfile
    ports:
      - "8000:8080"
    environment:
      - NODE_ENV=development
      - LOG_LEVEL=info
      - WORKFLOW_SERVICE_GRPC_URL=workflow-service:50051
    depends_on:
      - workflow-service
    networks:
      - youtube-transcriber
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s


  # Workflow Service - gRPC server with HTTP fallback for backward compatibility
  workflow-service:
    build:
      context: .
      dockerfile: ./services/workflow-service/Dockerfile
    ports:
      - "8004:8004"    # HTTP port (backward compatibility)
      - "50051:50051"  # gRPC port
      - "9229:9229"    # Node.js debug port
    environment:
      - NODE_ENV=development
      - HTTP_PORT=8004
      - GRPC_PORT=50051
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_DEFAULT_MODEL=llama3.1:8b
      - LOG_LEVEL=debug
      - DEBUG=*
      - NODE_OPTIONS=--inspect=0.0.0.0:9229
      - DOWNLOAD_DIR=/app/downloads
      - OUTPUT_DIR=/app/output
    volumes:
      - ./services/workflow-service/src:/app/src
      - ./services/workflow-service/dist:/app/dist
      - workflow-downloads:/app/downloads
      - workflow-output:/app/output
    depends_on:
      - ollama
    networks:
      - youtube-transcriber
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Ollama - Local LLM server
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_MODELS=llama3.1:8b
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - youtube-transcriber
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s


volumes:
  ollama-data:
    driver: local
  workflow-downloads:
    driver: local
  workflow-output:
    driver: local

networks:
  youtube-transcriber:
    driver: bridge
