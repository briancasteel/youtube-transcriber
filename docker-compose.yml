services:
  # Redis - Message queue and caching
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis-data:/data
    networks:
      - youtube-transcriber
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3

  # API Gateway - Request routing and rate limiting
  api-gateway:
    build:
      context: ./services/api-gateway
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - NODE_ENV=development
      - PORT=8000
      - WORKFLOW_SERVICE_URL=http://workflow-service:8004
      - VIDEO_PROCESSOR_URL=http://video-processor:8002
      - REDIS_URL=redis://redis:6379
      - LOG_LEVEL=debug
    volumes:
      - ./services/api-gateway/src:/app/src
      - ./shared:/app/shared
    depends_on:
      - redis
    networks:
      - youtube-transcriber

  # Video Processor - YouTube video download and processing
  video-processor:
    build:
      context: ./services/video-processor
      dockerfile: Dockerfile
    ports:
      - "8002:8002"
    environment:
      - NODE_ENV=development
      - PORT=8002
      - REDIS_URL=redis://redis:6379
      - LOG_LEVEL=debug
      - DOWNLOAD_DIR=/app/downloads
      - OUTPUT_DIR=/app/output
    volumes:
      - ./services/video-processor/src:/app/src
      - video-downloads:/app/downloads
      - video-output:/app/output
    depends_on:
      - redis
    networks:
      - youtube-transcriber

  # Transcription Service - Audio transcription coordination
  transcription-service:
    build:
      context: ./services/transcription-service
      dockerfile: Dockerfile
    ports:
      - "8003:8003"
    environment:
      - NODE_ENV=development
      - PORT=8003
      - REDIS_URL=redis://redis:6379
      - LOG_LEVEL=debug
      - LLM_SERVICE_URL=http://llm-service:8005
    volumes:
      - ./services/transcription-service/src:/app/src
    depends_on:
      - redis
    networks:
      - youtube-transcriber

  # Workflow Service - Orchestrates the complete transcription pipeline
  workflow-service:
    build:
      context: ./services/workflow-service
      dockerfile: Dockerfile
    ports:
      - "8004:8004"
    environment:
      - NODE_ENV=development
      - PORT=8004
      - REDIS_URL=redis://redis:6379
      - VIDEO_PROCESSOR_URL=http://video-processor:8002
      - TRANSCRIPTION_SERVICE_URL=http://transcription-service:8003
      - LLM_SERVICE_URL=http://llm-service:8005
      - LOG_LEVEL=debug
    volumes:
      - ./services/workflow-service/src:/app/src
    depends_on:
      - redis
      - video-processor
      - transcription-service
      - llm-service
    networks:
      - youtube-transcriber
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Ollama - Local LLM server
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_MODELS=llama2:7b
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - youtube-transcriber
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # LLM Service - Whisper + Ollama integration
  llm-service:
    build:
      context: ./services/llm-service
      dockerfile: Dockerfile
    ports:
      - "8005:8005"
    environment:
      - NODE_ENV=development
      - PORT=8005
      - REDIS_URL=redis://redis:6379
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_DEFAULT_MODEL=llama2:7b
      - LOG_LEVEL=debug
      - WHISPER_MODELS_DIR=/app/models
      - UPLOAD_DIR=/app/uploads
      - OUTPUT_DIR=/app/output
    volumes:
      - ./services/llm-service/src:/app/src
      - whisper-models:/app/models
      - llm-uploads:/app/uploads
      - llm-output:/app/output
    depends_on:
      - redis
      - ollama
    networks:
      - youtube-transcriber

volumes:
  redis-data:
    driver: local
  video-downloads:
    driver: local
  video-output:
    driver: local
  ollama-data:
    driver: local
  whisper-models:
    driver: local
  llm-uploads:
    driver: local
  llm-output:
    driver: local

networks:
  youtube-transcriber:
    driver: bridge
